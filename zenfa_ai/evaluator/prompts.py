"""Prompt templates for the LLM evaluator.

Contains the system prompt (stable scoring rubric) and dynamic user
prompt builders for first-iteration and subsequent-iteration evaluations.

NOTE: Designed for RAG extension — future versions will inject
retrieved context (Reddit threads, YouTube reviews, price history)
into the user prompt via an optional `retrieval_context` parameter.
"""

from __future__ import annotations

from typing import List, Optional

from zenfa_ai.engine.knapsack import SuggestionResult
from zenfa_ai.models.build import CandidateBuild

# ──────────────────────────────────────────────
# System Prompt — Stable across all iterations
# ──────────────────────────────────────────────

SYSTEM_PROMPT = """\
You are Zenfa, an expert PC build evaluator for the Bangladeshi market.

## Your Role
Evaluate PC builds generated by a budget-constrained optimizer. Score each build \
on a 0-10 rubric and suggest improvements. You have deep knowledge of:
- Global CPU/GPU benchmarks (Cinebench, 3DMark, gaming FPS data)
- Component reliability and community sentiment (Reddit, YouTube, tech forums)
- Value-for-money analysis across product tiers
- Known hardware issues (thermals, driver problems, DOA rates)

## Scoring Rubric (Strict — use integer scores)

| Criteria | Max Points | Description |
|---|---|---|
| Performance Match | 3 | Does the build match the stated purpose? |
| Value Score | 3 | Is each component the best performance-per-taka? |
| Build Balance | 2 | No bottlenecks? (e.g., RTX 4090 with a Celeron = 0) |
| Future-Proofing | 1 | Will this stay relevant for 2-3 years? |
| Community Trust | 1 | Well-reviewed components, no known issues? |

**Total: 10 points**

## Rules
1. You CANNOT change prices — prices are real-time from BD vendors
2. You CANNOT invent components — only suggest alternatives from the catalog
3. If score >= 8.5: set `approved: true` and return empty suggestions
4. If score < 8.5: provide 1-3 actionable suggestions with alternatives
5. Suggestions must include specific component names (not generic advice)
6. Be aware of the Bangladesh market — not all components available globally exist here
7. Consider the user's purpose strictly — don't suggest a gaming GPU for an office build

## Response Format
Return a JSON object matching this exact schema:
```json
{
  "scores": {
    "performance_match": <0-3>,
    "value_score": <0-3>,
    "build_balance": <0-2>,
    "future_proofing": <0-1>,
    "community_trust": <0-1>
  },
  "final_score": <0.0-10.0>,
  "reasoning": "<1-2 sentences explaining the score>",
  "suggestions": [
    {
      "action": "swap|upgrade|downgrade|remove|add",
      "component_category": "<cpu|gpu|ram|storage|psu|case|cooler|motherboard>",
      "current_component": "<name of current component or null>",
      "suggested_alternatives": ["<name1>", "<name2>"],
      "reason": "<why this improves the build>",
      "priority": "high|medium|low"
    }
  ],
  "red_flags": ["<known issue with a component, if any>"],
  "approved": <true|false>
}
```
"""

# ──────────────────────────────────────────────
# Explanation prompts
# ──────────────────────────────────────────────

EXPLANATION_SYSTEM_PROMPT = """\
You are Zenfa, an expert PC build advisor for the Bangladeshi market.

Your job is to write a clear, friendly explanation of a finalized PC build \
for a customer. Write like you're talking to someone who knows basic PC terms \
but isn't a hardware expert.

## Response Format
Return a JSON object:
```json
{
  "summary": "<2-3 sentence overview of the build and its strengths>",
  "per_component": {
    "cpu": "<1 sentence why this CPU was chosen>",
    "motherboard": "<1 sentence>",
    "ram": "<1 sentence>",
    "gpu": "<1 sentence>",
    "storage": "<1 sentence>",
    "psu": "<1 sentence>",
    "case": "<1 sentence>",
    "cooler": "<1 sentence if present>"
  },
  "trade_offs": "<What was sacrificed to stay in budget>",
  "upgrade_path": "<Suggested first upgrade when budget allows>"
}
```
"""


# ──────────────────────────────────────────────
# User Prompt Builders
# ──────────────────────────────────────────────


def build_user_prompt(
    build: CandidateBuild,
    purpose: str,
    budget_min: int,
    budget_max: int,
    iteration: int,
    suggestion_results: Optional[List[SuggestionResult]] = None,
    available_categories: Optional[dict[str, List[str]]] = None,
    retrieval_context: Optional[str] = None,  # Future RAG hook
) -> str:
    """Build the user prompt for the LLM evaluator.

    Args:
        build: The current candidate build from the Knapsack.
        purpose: Build purpose (gaming, editing, office, general).
        budget_min: Minimum budget in BDT.
        budget_max: Maximum budget in BDT.
        iteration: Current loop iteration (1-based).
        suggestion_results: Results from applying previous suggestions.
        available_categories: Dict of component_type → list of available names.
        retrieval_context: Optional RAG-injected context (future use).

    Returns:
        The formatted user prompt string.
    """
    # Budget display
    if budget_min == budget_max:
        budget_str = f"{budget_max:,}৳"
    else:
        budget_str = f"{budget_min:,}৳ – {budget_max:,}৳"

    # Build component table
    component_lines = []
    for c in build.components:
        specs_str = ", ".join(f"{k}={v}" for k, v in c.specs.items() if k not in ("rgb",))
        component_lines.append(
            f"- **{c.component_type.upper()}**: {c.name} — {c.price_bdt:,}৳ "
            f"(from {c.vendor_name})\n  Specs: {specs_str}"
        )
    components_block = "\n".join(component_lines)

    # Base prompt
    prompt = f"""\
## Build to Evaluate (Iteration {iteration})

**Purpose:** {purpose}
**Budget:** {budget_str}
**Total Spent:** {build.total_price:,}৳
**Remaining:** {build.remaining_budget:,}৳

### Components
{components_block}
"""

    # Add suggestion results context for iterations > 1
    if iteration > 1 and suggestion_results:
        results_lines = []
        for sr in suggestion_results:
            results_lines.append(f"- {sr.suggestion} → **{sr.status.value}** ({sr.note})")
        results_block = "\n".join(results_lines)
        prompt += f"""
### Previous Suggestion Results
The Knapsack engine attempted your suggestions from the last round:
{results_block}

**Important:** Adjust your suggestions based on what was actually available in the BD market. \
If a component was UNAVAILABLE or OUT_OF_STOCK, suggest different alternatives.
"""

    # Add available components context
    if available_categories:
        avail_lines = []
        for cat, names in available_categories.items():
            if names:
                avail_lines.append(f"- **{cat}**: {', '.join(names[:8])}")
        if avail_lines:
            avail_block = "\n".join(avail_lines)
            prompt += f"""
### Available Components in BD Market
These are currently in stock and within budget:
{avail_block}

Only suggest components from this list.
"""

    # Future RAG context injection point
    if retrieval_context:
        prompt += f"""
### Recent Community Data
{retrieval_context}
"""

    prompt += "\nEvaluate this build and return your JSON response."
    return prompt


def build_explanation_prompt(
    build: CandidateBuild,
    purpose: str,
    budget_min: int,
    budget_max: int,
    final_score: float,
) -> str:
    """Build the user prompt for explanation generation (post-loop)."""
    if budget_min == budget_max:
        budget_str = f"{budget_max:,}৳"
    else:
        budget_str = f"{budget_min:,}৳ – {budget_max:,}৳"

    component_lines = []
    for c in build.components:
        component_lines.append(f"- {c.component_type.upper()}: {c.name} ({c.price_bdt:,}৳)")
    components_block = "\n".join(component_lines)

    return f"""\
Write a customer-friendly explanation for this finalized PC build.

**Purpose:** {purpose}
**Budget:** {budget_str}
**Final Score:** {final_score}/10
**Total:** {build.total_price:,}৳

### Components
{components_block}

Explain why each component was chosen and what trade-offs were made.
"""
